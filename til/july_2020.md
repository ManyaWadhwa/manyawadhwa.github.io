---
layout: default
title: July 2020
---

### 5th July 2020

##### Mostly ACL notes

Tutorial notes:
* interesting research question: interpretability analysis of multimodal documents. How do representations learn from different modalities?
* Only speech analysis: some things are directly applicable from images, because they are also continuous signals -  saliency maps, spectrograms etc.
* work on interpretability on formal languages. trade off between the interpretability of the model and it's quality, you can constraint the model and gain interpretability but might lose the learning power..
* What we want, ultimately is for interpretability to improve the model, and not constrain it..
* In some situations interaction between the system and the user is important, whereas in other cases it might not be desirable or possible.. in a radiology report you'd want it to be a interactive, but for an auto-driving car, you'd want the model to be very sure and maximize performance instead of having "doubts"
* In NLP the issue of learning can be multi-step: model, data, the human language is not supposed to learn it? from the scientific point as well as from a linguistic perspective..
* how infinitely recursive should language be??
* Question:which probing classifier to use? is it fair to compare two classifiers with different sizes?
** Idea: of using the minimum description length tells you not only the performance of the probe, but also how difficult it is to solve the task, a measure on how challenging it is to do?
* human error patterns vs model error patterns : any work? how do you differentiate - and how do you use them to make your model more faithful? before you fix the mistakes, you need to see how they arise. * look at generation track papers at ACL
* Brilliant BERT visualization demo [here](https://exbert.net/) [IMPORTANT]
* Another point for probing classifiers was to learn the "latent variable" that causes the model to perform well i.e. more in terms of the control experiments.
* what about carefully controlled test sets as used in psycho-linguistics human studies? can you transfer these evaluations to models?  
** idea is to eventually use these small sets. Look up paper on : posing fair generalization tasks, emnlp 2019. With small corpus, you get 1 or 0 answers as to whether the model does good on it or not, whereas with numbers on bigger corpus it's very difficult to gauge which quality of the model is being assessed. But again the trick is to ensure the very small targeted corpus is created properly?
* Q: What does it even mean to say that "a pretrained LM knows a task at 80%"? to me it sounds ;like the model DOES NOT know the phenomena :
** We want to go from not knowing it, to knowing it a little, saturating on the information. Similar to how it's in humans, kids might be not knowing a phenomena, and then they go on to knowing it properly. In models, the all or nothing doesn't happen, so how do you say is "80%" good enough? On going from 80 - 81% you dont know what the model learned in that 1%.... VERY VAGUE HERE.
* thoughts on probing for factual knowledge? decouple what the model can be trained on vs what the model can learn .. decouple memorization from reasoning .. memorization vs inference. The model needs to be more than a look up table, and more than just a replacement for a KB.
* What is your thought about configuring probes for fairness or privacy-preserving purposes, e.g. to show a model lacks interpretability wrt to gender or other protected attributes when it's trained to some reasonable degree for some designated prediction task?
** Very important, but how do you define biases? some exist in society and it is very difficult to get rid of them. It's definitely a good use case (look up references). Connects probing with the behavior side of inference.

Session 2 :
* disagreements in human judgements, how to handle these? inferences about meaning when there is no clear cut answer .. hard to say what behavior our model is supposed to have given these uncertainties.. challenge on how to design these sets
* do we always want to mimic human behavior? depends on the goal of the user creating the system .. eg: dialogue systems are difficult to evaluate, but for something like question answering you can still be more objective and try and achieve "super human" performance
* Q: Interpretability & subwords: how (if) do you think the fact that SOTA contextualization models use byte pairs affects their interpretability when compared to "standard" LSTMs + word-tokens
** we as humans don't use subwords and these don't necessarily adhere to the morphological information, and that makes it a little bit harder to interpret ( from a visualization perspective ). For behaviourial analysis : ends up being a part of the black box, the advantage of subwords is in the multi-lingual languages ( moving away from white space tokenization ). Investigation of how these different subwords change how the models performs ( subpiece vs word piece etc ).
* Q: Do any of you have thoughts like these? Are you tempted by models which can be better "guided"?  Any insights?
** with models now you try and build tasks into a model, which is mostly human guided < look up more about it later READ >
* Q : Which type of interpretability method (like post-hoc or model based) should be better for educational NLP areas like essay scoring, etc.
** Is the audience the engineers or the students? if it's the first, everything still remains the same, other than the document based models. If the audience is the students - still nervous using the tools - so mostly visualizations. The structural and behavior based methods are more for the machine learning engineers. Check for systematic biases while scoring students. What type of explanations to show to the students? ( from a user perspective )
* Q: Have you seen any work performing probing studies on document representation (say from BERT)?
** Not really. looked at discord representations, generation settings to say what are they sensitive to .. what sort of coherence chains to they have or not have .. You need adaptation to the model though.
* Q: How does one analyze a model architecture which transitions in terms of "what" it does [in terms of various linguistic properties] with increasing or decreasing training/finetuning sizes? [e.g it encodes property z only after a certain threshold of training examples N_min, or perhaps worse still, the behaviour is non-monotonic w.r.t encoding z]
** maybe by looking at different checkpoints, but at different points in training, diff datasets etc. Very complex to look at - can't just look at it post-hoc as an exploratory analysis.. probably better driven as hypothesis testing ( eg: Certain linguistic understanding gets better with training time etc )
* What's your thought on recent debiasing methods that work by downweighting potentially biased examples in the training data? They work well in improving the performance on some challenge datasets (e.g., HANS). But since new types of bias are still being discovered and these methods usually target a specific 'known' bias -- do you think these methods are the way to go for a better and more robust model of language?
** Similar to data augmentation, but skeptical of these kinds of methods - the concern is that these are too tight to the probing set - we need a much fuller picture of what did the downweighting change, but we need to look at a more holistic picture, what other heuristics it might be adapting.
* Q:Applying current probing methods to long documents?
** Analysis methods are limited by common tasks in NLP, and there aren't many common tasks looking at full documents. Generally stay in sentence level, or paragraph levels. The only popular tracks are summarization.. It is intrinsically much harder to do this cause you don't know how to fully interpret a model.

#### Second tutorial :
[Common Sense Tutorial](https://homes.cs.washington.edu/~msap/acl2020-commonsense/)

##### Common sense knowledge in pre-trained models: ( generic common sense )
* Q: Do pretrained models already capture common sense knowledge? ( only during pretraining ) : there is some evidence from KB completion tasks // BERT performs better on one-to-one answers as compared to many-to-many, and doesn't perform as well as supervised methods ( expected ).
* Weir et al, 2020 - properties of concepts: can the LMs distinguish "concepts"? Models perform better on encyclopedic / functional knowledge as compared to visual or perceptual knowledge. Difficult to learn these alone from text. Positive evidence that there is knowledge in the LMs.
* Can we trust these models? Not always maybe ..
* Zero shot setup for LM common sense training vs knowledge informed model
* knowlede informed model: with each question, enrich it with more information from conceptnet, ngrams, come. Improves the model but clarifications are not useful as per humans
* good performance is due to knowledge in LM or training the large model with more data?
* Do NLM representations learn physical commonsense?
* mitigate reporting bias? look at surprising outcomes etc/ [read van durme's paper]

##### Common sense resources / existing efforts to distill knowledge in the current models
* "mental models" [graesser 1994] + personal experiences + world knowledge and common sense
* common sense: bank of knowledge that the models can use
* existing resouces : cyc ( lenat et al, 1984 ) - human like common sense reasoning and develop new rules etc . Developed their own ontology and language with new concepts and a reasoning engine
* conceptnet? : [ question : using concept net for entity linking or hierarchical entity linking ] has general common sense knowledge, multi-lingual. Question is concept net multi-modal? this is more semantic knowledge
* atomic: causes and effects of action triples, more inferential knowledge
* extracting information:read and parse, create rules, and filter them..
* framenet?

##### NN models for common sense reasoning :
* KB + text -> model -> output ( informed models )
* incorporating external knowledge into Neural Models
** task : what task are you solving and if you need external knowledge? ( story telling, machine comprehension etc)
** KB : where are you getting your knowledge source ( conceptnet, atomic, wordnet, sentiwordnet, cyc, mining, handcrafted rules etc )
** neural component
** combine information sources? ( scoring function, convert symbolic to vector representation (+ attention ), multi task learning )

Q Does fine tuning make the LM unlearn the common sense it learnt during pretraining? "catastrophic forgetting"

##### Neuro symboli representations of commonsense knowledge

* Limitations of knowledge graph:
** very rarely do you find the same query as is, so you end up return knowledge that's incorrect or noisy
** problem with common sense knowledge is that it's often implicit and not really written down. So you started using big graphs like conceptnet
** learning structure of knowledge: is to < head, relation, target> make the language model ( now known as "knowledge model" ) to generate the target, after seeing head and relation // fine tune a pretrained model to the above training style
** common sense transformers OR "COMET for short
** Comet model : atomic knowledge graph // transfer learning from language



### 4th July 2020
##### Mostly ACL notes
continuing on my TILs from "Interpretability and Analysis in Neural NLP" hosted by Yonatan Belinkov, Sebastian Gehrmann and Ellie Pavlick...


##### behavior analysis
the second way of understanding interpretability of these models is to a behavior analysis. In this method, you tend to look at  data points that are statistically less probable to be seen my the model during training, and then you try and see how your model generalizes to these points. This approach is very different from the probing or structural analysis approach.
* Some advantages: it's algorithm agnostic, practical, has an objective criteria for evaluating the representations.
* disadvantage: 1. if the model doesn't perform well on the curated dataset - do you debug the model or the data it's seen? 2. what level of generalization is considered to be fair? 3. you technically don't get much insights about WHY the model failed.

##### visualizations :
The third method for interpretability is via interfaces and visualizations. You try and understand the larger patterns by filtering noise.. this can help understand concepts in higher dimensions. It can be used for reducing exploration space ( eg; in model selection ), understand data, features, as well as important aspects of the modeling concepts ( eg: attention layer )

Can look at it in three perspectives:
* task : model selection , model decisions
* user : architect ( who sees just the model ), trained ( model + data ), end-user ( only sees the application )
* model involvement : interactive visualizations( with has the ability to change model parameters, links via interface ) , passive involvement ( eg: in tensorboard you only see the statistics, graphs etc but have no control over the actual underlying model  )

* difficulties: takes a lot of time and effort to build these interfaces especially to scale over multiple usecases
*  open research questions :
** how do these visualizations improve downstream trust?
** how do these insights improvement the model ? ( eg: visualizing the outliers )
** how to help understand causality?
** how to develop model interactive generic interfaces?



### 3rd July 2020
##### Mostly ACL notes
So I haven't posted in a while, because I was attending two conferences along with office work. I'll try and be more diligent this week, and write about my TILs regularly.  

In the coming week I'm attending ACL 2020! Seems very overwhelming given the number of tutorials, workshops and the sheer number of papers that have been accepted!  A lot of time this week has gone into trying to figure out what sessions I want to attend. Since the tutorials are recorded, I went through the tutorial on "Interpretability and Analysis in Neural NLP" hosted by Yonatan Belinkov, Sebastian Gehrmann and Ellie Pavlick.
Main takeaways:
* This tutorial focused on three ways for Interpretability : structural analysis,
* The first part of the talk focused on structural analysis via "probing classifier".

##### probing classifier

TLDR for a probing classifier:
```
Input: x, linguistic_ann(x)
F: f(x) -> rep(x)
G: g(rep(x)) -> g_output
evaluation g_output with linguistic_ann(x)
```
* In this model, you basically evaluate F, and the representations generated by model F on a dataset with has some linguistic annotations. Another model "g" is then trained using these representations to see how it performs on the same dataset. Model g is trained in a way to maximize the mutual information between the representations and the output annotations
* on looking at the performance of "f" i.e. representations from intermediate layers of "f" , we can conclude a hierarchy of tasks that each level would represent better.
* i.e. the lower layers of any NN model learn  more simple features like morphology, POS where as the upper layers can capture more complex concepts like dependencies, syntax and semantics etc. ( similar to as seen in CNNs for vision for example)
* shortcomings of this approach:
** even though we are comparing rep(x), but the performance is also based on how to choose "g", so the more complex "g" you choose, the better you are able to use the representation for the linguistic task. how does that play into comparing different "f"?
**  you need to have some standard models of "g" for comparison to say how the representations stand relative to a baseline
** the model "g" is being trained on a linguistic task which may be different from the actual task on which "f" is trained so even though "f" is performing well on the probing task, it might not perform well on the actual downstream task, how to resolve this?
** difference between "accuracy" and "selectivity" <--- (read about selectivity)  

continued ..
