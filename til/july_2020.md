---
layout: default
title: TIL/July 2020
---


[Go to the main TIL page](https://manyawadhwa.github.io/til/)

## 31st July 2020

Trying to learn more about GPT-3 I came across a blog by Kevin Lacker : ["Giving GPT-3 a Turing Test"](https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html)

* I think like following up on some of the points below, this blog does exactly that - trying to see what the break point of GPT-3 is, and if the model learns any reasoning at all, or if it's just doing things by association / or a fuzzy look up because has seen tons and tons of things.
* Listing some points from the blog directly:

```
The lesson here is that if you’re a judge in a Turing test, make sure you ask some nonsense questions, and see if the interviewee responds the way a human would.
```

```
These wrong answers are actually fascinating! None of these were presidents of the United States, of course, since the US didn’t exist then. But they are all prominent political figures who were in charge of some US-related political entity around that time. In a sense, they are good guesses.
```

* To be continued the next week readings on :
* [GPT-3 Creative Fiction](https://www.gwern.net/GPT-3)
*

## 30th July 2020
* How do you measure if GPT-3 is intelligent or memorizing? hard to device a test for itself - mostly by example..
* Paper : ["Transformers as Soft reasoners over language"](https://arxiv.org/abs/2002.05867) by Allen AI
* come up with some framework for reasoning the behavior
* NN probes - which document is the most similar to this on top of a representation
* [Radioactive data: tracing through training](https://arxiv.org/pdf/2002.00937.pdf)
* They are very clever at taking a shortcut to the output, and the transfer flow of data might be a "watermark" or representative of your data
* [On the Measure of Intelligence](https://arxiv.org/abs/1911.01547)
* The model learns a taxonomy from a large data given small examples - not sure if it learns or memorizes : given a title, it interpolates things and is able to "query" things. So it might not know what concepts like "church" etc means, but it still has the taxonomy within it.
* does the model capture any timelines? Any sequence of events?
* no deduplication can still save you from repetitive things; some of these things are old and sketchy; data from 1950 might be different politically from what is today
* fake news generation with GPT-3: short time till it's available for public - maybe it's difficult for language as compared to videos?
* in classification there is stratification in the datasets, that's impossible to do in such a huge language model. Does it have more impact? If it's learning concepts, it might learning more concepts in one direction as compared to another? There is no way to balance these heterogeneous concepts such that they are balanced. Do we want to balance it?
* Any snapshot will always be out of date - it's evolving continuously - how to do you capture this time thing?
* reverse index : only want weights where only a certain subset matters.
* remove meaningfully a gender bias? 1. have a definition - things are still evolving?  
* a. world you have b. world you aspire to have ----> there is bias in both cases, you'd want to bias towards (b).
* De-biasing: These models and how we train  them are not representing the data as it occurs.
*  Biasing : The other field is saying that there is a state you want to achieve, how do you bias it.
* regularization can be one of the bias in models  ( READ paper)
* the state of the world is biased! :(
* The consciousness of the society changes - how do you encode it?
* "recency bias" -> since we have more data now, things are more from recent era - however we don't have enough data from previous era because we never recorded enough - how to fix this? ( read about this )



## 29th July 2020
* Past couple of days been reading blog entries and watching some youtube videos about GPT-3. Noting down some points which might be interesting. Also linking resources

* auto-regressive vs denoising autoencoder : GPT 3/ unidirectional vs BERT ( all words at the same time)
* The exact function being computed
* can humans really generalize to new situations OR do they take certain skills from other situations and transfer it?
* we generally do the latter, so can we come up with certain skills which can be fall back options for the system - where if it doesn't recognize something, it can use these skills?
* if this happens with GPT-3, where they see something out of distribution - they can at least generate something that is not improbable. Can it recognize out of distribution? And how does it handle it?
* if you give it something off the manifold - would it generate garbage? probably, because technically humans might do the same ( just manifold for humans is really big / vast ). Interesting thing:
* bidirectional model learns more about the structure, but maybe unidirectional is easy to learn at a BERT type scale.
* Transformers in both cases denoising and auto-regressive cases have a horrible quadratic increase in computation size with an increase in the number of tokens, because of which they had to truncate to 512 tokens. But MAYBE when you go to the scale of GPT-3 it doesn't make as much difference, cause everything is so big you have that much computation.
*  trade offs - of having longer input size - with LSTMs, it's a bad thing - because you can't remember it . CNN also has a receptive field - is there something similar with transformers?
* Problem is with correlated information - if you have correlated features it might have a problem because you wouldn't know what to pay attention to ?  QUESTION  : what does correlated input for text mean? ( 1:03:47)
* much longer receptive field, input more than one document - how do you put it together or what do you pass one after the other?
* INTERESTING POINT - not sure how to get this done - but if you could make certain layers of the document to tend to another document which is similar in nature - that would be awesome!!! Then you are basically giving it more context from "outside" document information which is also related to the same concepts. It's already sort of been done - building the logic and knowledge into the weight connections, what we are missing is memory modules within the model that can actually do this. ( Find papers that do this in any direction!!! )
* representation -> store in memory -> and during a forward pass let's say find these more documents that are similar to the input and maybe use these as extra context or knowledge to process the input ?
* reverse index between the representation and the input , see how the connections are activated - see which examples led to this. Explainability wise is probably done. Explain by example. It should be a next search engine - fuzzy search engine.


## 28th July 2020
* Ran entity linking with BLINK today!

## 27th July 2020
* A large chunk of my time this week went in running experiments for my office project. Learnt quite  a lot on how to iteratively build on a model and debug performance along with debugging input data format. Not sure if I can talk about on a public site, so will skip that aspect.

## 24th July 2020


## 23rd July 2020
I'm working on a siamese network sort of an architecture ( can't go into details), but basically multiple inputs, same model and then more downstream tasks. It's an interesting problem because on  paper it makes sense, and I have implemented a Siamese network in pytorch, BUT, i had to work with tensorflow and my entire day went in solving differences between tensorflow 2.1 and 2.2 For some reason with a pre-trained language model the 2.1 version was giving me compiling errors, where 2.2 wasn't. SO :O
Will spend most of tomorrow also figuring this out..

## 22nd July 2020
Continued notes on [Do NLP Models Know Numbers? Probing Numeracy in Embeddings](https://arxiv.org/pdf/1909.07940.pdf)
* Interesting probing tasks - list maximum, decoding from text to number, and addition - classification, regression and regression tasks respectively.
* BERT performs poorly on these tasks - struggles on large numbers and floats.
* ^Testing for above was done on numbers within the same range as the training data
* for outside range : all models struggle to extrapolate on the decoding and additional tasks, for the maximum task, the values are still similar as the interpolation (within range) but doesn't do well on large values
* augmenting data generally works
* relative ordering naturally emerges for certain concepts


## 21st July 2020
Add notes on [Do NLP Models Know Numbers? Probing Numeracy in Embeddings](https://arxiv.org/pdf/1909.07940.pdf)
* As the title suggests this paper probes for numeric understanding in embeddings for a couple of state of the art language models
* The main aim is that most of these language models / with embeddings treat numbers in text the same way as the other tokens - they are embedded in a distributed manner. But is this enough to capture things like 75 >>> 57 etc.
* They talk about a DROP dataset ( read more ) that investigates numerical reasoning capabilities of question answering models ( i don't have  a background in question answering models , but will learn more in the paper probably)
* they probe - BERT/Glove/ELMO on tasks like maximum, minimum, number decoding, addition.
* Abstract results - glove and word2vec accurately encode numbers upto 1000
* character level embeddings are best for all pre-trained models - but BERT using sub-word lists is less exact!!!! [TAKEAWAY]
*  Datasets for numerical reasoning : "DROP" and EQUATE
* models successfully exhibit numerical reasoning - min/max, argmax, comparison.

## 20th July 2020
Neural Architecture Search Problem
* How good are NAS heuristics?
* What algorithmic components matter?
* Can we do something simple to achieve state of the art?

Background
* Hyperparameter search is a super set of NAS
* What is Hyperparameter search? --- tuning a model, start with an architecture and you tune it, sometimes you are tuning the nodes etc so that's architectural, non architectural are objective function ( regularization parameters etc), but this a hard problem - deep learning models will take a long time to learn, a lot of hyper parameters for these big models etc
* NAS: instead of a fixed backbone, you learn the backbone itself - more complex search space, you don't consider non architectural parameters.
* intractable search problem
* what is being proposed? don't consider full generalizable search spaces // but instead : but restrict search to repeated blocks or "cell blocks"
* cell block search space & meta architecture space: we want to learn a particular DAG or fixed meta architecture
* takes a lot of compute time <-- fundamental limitations of the current SOTA
* ENAS and DART

* ENAS ( Pham et al. 2018 )
* motivated to speed up the first generation methods
* better perplexity compared to random search
* DARTS ( Liui et al.. 2019 )
* better perplexity as compared to ENAS - cleaner algorithmically

* How good are the NAS heuristics?
* What algorithmic components matter?
* Can we do something simple to achieve state of the art?

* Search Space that is continuous or discrete ( compared to just discrete in hyperparameter )
* search method , random search, continuously sample from search space
* evaluate? each of these configurations you fully train and see how it works.
* how can we "efficiently" identify "high quality" parameters? ( efficiently - time optimized, high quality - low perplexity)
* how can we speed up random search?
* don't consider training process as black box - you are ideally running an iterative training regime and you can get the intermediate models , some iterations are wasted, you want to find the really good model, if after some iterations there are models that are sub optimal, discard them , instead focus on good going models so you DOWNSAMPLE the models.
* how to discard a configuration? not downsampling too quickly and not being too conservative as well
* THEORY: multi-armed bandit problem
* PRACTICE : ASHA - asynchronous execution for massive parallelism / hyper ban
* change search space: ( cell block and meta architecture ), evaluation is now a on partial models
* ASHA is still random search space so can be a new baseline
* Why is DARTS doing so much better than ENAS or ASHA? What algorithmic components matter?
* diff search methods - random, evolutionary, bayesian, gradient based, reinforcement
* evaluation - weight sharing, hypernetworks, partial networks
* can do any combination no comparable comparison
* RSWS - random search with weight sharing : simpler is better
* algorithmically : Randomly sample architectures from the search space //  Sampling graphs from SS // sample edges / nodes eg: sampling either a convolution or pooling ; Question - do we add some rules in terms of pooling after convolution or some sort of order in the sampling?
* weight sharing concept: // in something like ASHA or normal methods - you generally learn models for all the configurations you have picked up. BUT that is very very expensive right? So you do weight sharing! Joining minimization problem! Instead of finding the best architecture you find a fixed set of weights for all architectures which on average are pretty good. Have a low loss on average. What does it mean to have shared weights across architectures - how do you assign weights to different architectures? READ MORE
* can these ideas be used for pretraining vs fine tuning? People haven't looked at it yet! Open question.
* over monte carlo search?

## 19th July 2020
Add notes on sentencepiece bs wordpiece ( albert vs bert vocabulary )

## 18th July 2020
Compiled notes

## 17th July 2020
Officce workk

## 16th July 2020
Add notes on [Hierarchical Attention Networks for Document Classification](https://www.aclweb.org/anthology/N16-1174.pdf)

##15th July 2020
Add notes on [Neighborhood Matching Network for Entity Alignment](https://arxiv.org/abs/2005.05607)

## 14th July 2020
[Biomedical Entity Representations with Synonym Marginalization](https://arxiv.org/abs/2005.00239)
* This paper was of interest because they tackle how to deal with generating negative samples especially for the case of linking with variations in surface names ( close to the problem of linking that i worked on - for organization names ).
* In this paper they focus on learning representations of biomedical entities solely based on the synonyms of these entities
* they have a unique way of avoiding pre-selection of negative samples, and leave it to the model to surface the most difficult negative samples ( based on how they are training )
* they use a synonym marginalization technique, which maximizes the probability of all synonym representations in top candidates, also dispenses the need to explicitly have negative training pairs.
* they use a combination of sparse and sense representations to capture morphology as well as semantics.
* I think it's interesting because in an entity linking task also, it is difficult to come up with challenging "candidates" / "negative" samples, because anything other than the correct entity is essentially negative. How this helps is it learns contextually and morphologically similar but negative samples
* They use "Maximum Inner Product Search" ( read about this ) for retrieving the concepts represented as sparse and dense vectors

* Model
* encode the mention, and synonym using a shared encoder. Iteratively update the top candidate and calculate the marginal probability of the synonyms based on their representations.
* Sparse representation : tf-idf, character level ngram statistics computed over all synonyms.
* Sparse scoring function :

```
 similarity_sparse =  sim_func(sparse_mention, sparse_entity)
 ```

 * dense representation: BERT encoding / dense scoring function is the same as above, but with dense encodings.
 * final function

 ```
 similarity = similarity_dense + weight * similarity_sparse
 ```

 * training : use a model based candidate retrieval and maximize the marginal probability of the positive synonyms in the candidates
 * they do an iterative candidate retrieval - where they sample some candidates from the sparse representation and some from the dense representation. Since the sparse representations are static, the top n for that remains the same, but since the dense encoder is changing, the candidates in each retrieval differ as the model learns better dense representations.
 * given top candidates, they now maximize the marginal probability of the positive pair - which they call "synonym marginalization"
 * their loss function is interesting , i'm wondering if it generalizes for other problems ( should if the setting is similar )
 * At inference they compute the representations and retrieve the nearest one using MIPS ( which I think is just cosine distance ) and you pick the nearest one.

## 13th July 2020
* Read two papers back to back about entity linking ..
[LEARNING CROSS-CONTEXT ENTITY REPRESENTATIONS FROM TEXT](https://arxiv.org/abs/2001.03765)
* This work comes out of Google research, so pretty good. Different approach compared to the BLINK paper downthere.
* They train a model like a normal LM in a fill in the blank manner, and see how it generalizes to different tasks, so this is not specifically trained for entity linking
* in this paper they learn the fill in the blank tasks to learn context independent representations of entities from the text contexts in which these entities were mentioned. They learn "high quality entity representations"
* They hypothesize that by learning an entity encoder which aggregates all of the textual contexts in which an entity is seen, we should be able to extract and condense general purpose knowledge about that entity
* "RELIC": Representations of Entities Learned in Context, it's  table of independent entity embeddings that have been trained
* they say that they can capture categorical information as less as wikipedia hierarchies.
* does entity linking without using any entity linking specific features ( like external knowledge bases / alias tables etc )
* it learns better representations if it is trained to match the context in which entities are mentioned and not trained to match the mention form itself.
* can be stored as an embedding matrix
* The method / setup is simple - they use a transformer encoder to embed each context initialized with BERT. For the entity, Each entity is mapped to an ExD matrix - in the loss function :

```
scaled_cosine_similarity = a * cosine(entity_embedding, context_embedding)

a -> scaling factor, is a learned parameter

p(entity/context) = softmax(scaled_cosine_similarity)

and they maximize the average log probability above.
```

* In order to keep the computations under bound, they use a contrastive loss - to sample k negative entities from a noise distribution.
* For the entity linking problem, they create a context from the document's first 64 tokens as well as the 64 tokens around the mention to be linked -- this is done with the assumption that in description documents the introduction will tend to be more dense.
* They have a section of the effect of masking on different downstream tasks - so masking mentions during training is beneficial for entity typing tasks, but not so much for the entity linking tasks.
* difference being in entity typing you tend to concentrate more on the context / the task relies more on the context compared to linking where modeling the mention surface forms are essential for linking.
* They do further analysis on more tasks like question answering , entity typing, few shot reconstruction tasks etc but mostly entity linking was of interest in this paper for my tasks at work



## 12th July 2020
Read : [Zero shot entity linking with density entity retrieval](https://arxiv.org/abs/1911.03814)
* They consider a zero shot entity linking challenge
* setup: each entity is defined by a short description
* model reads these descriptions along with mention context to make final linking
* Two stage approach : 1. retrieval in dense space defined by a bi-encoder that independently embeds the mention context and entity descriptions: what is a bi-encoder? 2. cross encoder that concatenates the mention and entity text.
* I think the takeaway is how they do retrieval , in a dense space instead of a heuristic / look up based system. They achieve high recall on the retrieval.
* they use entity descriptions
* transformer based bi-encoders for candidate generation and cross-encoders for ranking
* They assume each mention has a valid entity in the KB and don't handle out of KB cases.
* they test zero shot linking // unseen test KB
* the setup is :

```
Bi-Encoder:
y_m = reduce(transformer_1(mention_input)) ## mention input is mention and context together
y_e = reduce(transfer_2(entity_input)) ## entity input is entity and description together
```

* The above statement you basically have two transformer networks, one for mention and the other for entity + descriptions
* The optimization is done of the dot product:
```
loss(mention, entity) = -similarity() + log(sum_over_batch(similarity()))
```
* trying to minimize the loss i.e. maximize the similarity between the correct (mention, entity) while minimizing the similarity between the incorrect (mention, entity_all) pairs in the same batch.
* This is a very compute heavy task, so they create an index using FAISS ( read about it ) that is used for quick retrieval in a large-scale setting
* The next setup is the cross-encoder

```
combined_representation = reduction(transformer(mention|entity))
```

* This basically generates a combined representation of the mention and entity to give a score as the output. The optimization is similar to the above,  where you reduce the combined representation with a linear layer, and then softmax it .
* SO in the end they have the biencoder as the retriever : which is basically a highly semantic / context aware space and you can just retrieve top "k" as candidates using this.
* The second step is essentially the "re-ranking" of these retrieved entities w.r.t to the mention and it's context.
* All of the above done using BERT ...

## 11th July 2020

* Less learning, more engineering day I guess. But switched over from pytorch to tensorflow 2.2. Watched some videos, saw some primary differences:
* still have graphs
* tensorflow v1 we manually implemented a graph and started a session.. however as a developer now with tensorflow 2.0 we no longer need that!
* sequential API easiest to start with - built in layers etc, can do model.compile to do compile checks, can pull layers out of it and get more intermediate functions.
* can move to functional API if needed as well and move to more complexity - for building directed graphs,
* READ: gradient tape in tensorflow v2.0
* cache lines to make easier preprocessing and keep it in memory
* distributed training // mirroring strategy : multi GPU system


### 10th July 2020
Workshop day 2 at ACL! Still compiling my notes...

### 9th July 2020
Workshop day 1 at ACL! Still compiling my notes...

### 8th July 2020
Third day of ACL! Still compiling my notes...

### 7th July 2020

Reading List for this day is mentioned [here](july_2020_acl_paper_list.md)

#### Paper 1 : [BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance](https://www.aclweb.org/anthology/2020.acl-main.368.pdf)
-- initially I didn't know that this concept by the same authors has already been submitted to AAAI'2020. Interesting. But nice! Maybe they did some more work on top of their simple attentive mimicking algorithm
*  BERTRAM : BERT for attentive mimicking - understand rare words better
* Dataset rarification : transform datasets such that rare words are important for solving the task.
* the main idea of the paper is that because of the way BERT tokenizes the words there is a loss of information, which makes it difficult for it to perform on rare words.. eg: unicycle will be broken into : un, #ic, ##y, ##cle so the authors argue that there is a loss of information in representing the word in a distributed manner, and instead should be represented initially in a non contextualized way!
* One of the ways these can be used as input ( non contexualized embedding ) eg: "the child is riding a un #ic ##y ##cle / unicycle"
* So instead of passing just the tokenized word or full word - you can "/" it and send.
* QUESTION:how do you identify a word to be "rare" from an inference perspective? Answer from the chat: any word that occurs < 100 times in the BERTs pretraining corpus can be considered as rare..
* Model: to get the representation of the rare word, you [mask] it in the input sentence, send the sentence through BERT to predict the word, and pass it through a linear model to get an embedding. In the input you also send "ngrams"  of the word to have "form" information.

```
For one word in one context:
Form Model : form([word_nrgams]) -> v_form
BERT : bert([v_form, w_1, w_2...[mask],..w_n]) -> v_word_in_context

For one word in multiple contexts:
Attentive Mimicking: attention([v_w_in_c1,v_w_in_c2 ... v_w_in_cn]) -> v_final
```

* Attentive Mimicking : is basically a linear combination of all the context embeddings, where the weights are determined using self-attention.
* How to train? Use words for which we know BERT performs well, and pass it through the BERTRAM model - where the loss function is the difference in the embeddings produced by the model and the gold standard embedding.
```
loss_function = BERTRAM_embedding - gold_standard_embedding
```

* they say the model performs well as long as the context is not more than 128 words - why this limit?
* relevant papers from the chat - https://arxiv.org/abs/2005.04611

#### Paper 2:[SenseBERT: Driving Some Sense into BERT](https://www.aclweb.org/anthology/2020.acl-main.423.pdf)
* Driving some sense into BERT
* Infusing more semantics in NN
* BERT is pretrained model - MLM
* But even though BERT is contextualized it is not able to capture super granular sense - the same word can have multiple meanings i.e. "senses"
* trained a masked sense prediction task along with masked word prediction* use wordnet to induce weak supervision - along with noun - what sort of a noun is this? noun - person, food etc LEXICAL CATEGORIZATION TASK
* wordnet gives a "word to sense" mapping
* "output input weight tying trick" : [paper](https://arxiv.org/pdf/1611.01462v3.pdf)
* does the model just learn these senses? the masking and training is done in a way that the model's understanding on the sense is re-inforced by showing the same word in multiple senses and by showing the same sense for multiple words/context


#### Paper 3: [SpanBERT: Improving Pre-training by Representing and Predicting Spans](https://virtual.acl2020.org/paper_tacl.1853.html)

I think I attended Danqi's talk at AAAI at a NLP for Question Answering workshop where she spoke about this work!
* Designing more efficient pretraining task
* Why do we need spans? - global named entities / local or domain specific meaning.
* Span representations are also ubiquitous in NLP - QA, relation extraction, coreference resolution etc
* Pretraining with Span representations : contiguous masking spans,SBO - span boundary objective , single sequence document level input pipeline
* the objective function now looks like the following :
```
loss(word) = loss_mlm(word) + loss_sbo(word)
loss_sbo = -log p(word/span_start, span_end, relative_pos_of_word)
```
* do the above for each span word
* trying to encode the span into it's end points
* single sequence inputs compared to bi sequence inputs in BERT
```
single sequence input : [cls] <input_sequence> [sep]
bi sequence pipeline : [cls] <sent> [sep] <sent> [sep]
```
* the second one might lead to conditioning on noise ( why and how )- because it is randomly sampling from the document ..
* document level sequence input provides more context for longer spans

---

### 6th July 2020

Reading List for this day is mentioned [here](july_2020_acl_paper_list.md)

The main conference ACL started on the 6th of July! Very overwhelming, but in a way nice cause it gave me access to A LOT  of resources and mentoring sessions.

Some highlights:

#### keynote session by Kathleen McKeown
* She spoke about language generation - past present and future.
* Aspects of past relevant in the deep learning age?
* language generation can be : data, summarization, dialog, and machine translation ( interesting and true! )
* Neural Nets are the only way in which we know how to approximate a non linear function.
* Pre deep learning approaches: in the 1990s papers in generation focused on learning like the use one word - which is very interesting ( eg: use of "now" in the temporal sense vs the use of "now" in the discourse sense ), the evaluation wasn't as intense as it is now these days, it was done by looking and manually looking at the output.. then evaluation moved to developing annotations and corporas.
* Data - all the variation is in the long tail distribution, but mostly everyone ends up evaluating on the head , Original vision on frame semantics are not just semantic role labeling but included a deeper pragmatic understanding  
* Applications for which deep learning is not suited: < personaly for me these were the most interesting highlights ) - constraints on choice in language generation, looking at data, learning from other disciplines, and SOLVING PROBLEMS THAT MATTER AND NOT JUST PROBLEMS FOR WHICH DATA IS AVAILABLE.
* interesting ideas: controlled generation, include constraints while generating language, generating the "plan" first and then generating the language from it - so more like a prototype and then you actually take the path, old-fashioned AI dialogue planning - in this context you can produce x dialogues, how to you deal with spurious correlations, using "symbolic" or "semantic" logic, understanding causality is very easy to understand for humans, events in time WITHOUT having to update the models with weeks of training - easy update to the knowledge, solving a particular task and not just a dataset seems to be a huge challenge still..

#### Papers

SO the first day of ACL has many many many papers, I tried to go through some of these so these are the initial set of notes, will go through the others properly in some time:

#### Paper 1: [Moving Down the Long Tail of Word Sense Disambiguation with Gloss-Informed Biencoders](https://arxiv.org/abs/2005.02590)

One of the reasons this paper seemed interesting was because in the abstract it stated the following "word senses are not uniformly distributed, causing existing models to generally perform poorly on senses that are either rare or unseen during training."

Which even though we moved from word2vec to more contextual embeddings, the above is a problem that that even contextual models face..

* Word Sense Disambiguation task : Given a word and the context, choose the appropriate definition.
* word senses have a zipfian distribution ( read )
* data imbalance because of uncommon words..
* capture rare sense with pretrained models and "glosses"
* So there has already been work done with glossBERT, or having a neural encoder for the gloss BUT shortcomings
* Gloss informed Bi-encoder : encode the context and gloss independently and align the target word embedding to the correct sense embedding - biencoder takes two independent BERTs and encodes the context as well as the gloss independently. More efficient than 'cross encoding' since it the transformer performance is quadratic w.r.t the length of the input..

```
Context Encoder : f(input: context + word)-> emb_context
Gloss Encoder : f(definitions) -> emb_sense_1, emb_sense_2 ... emb_sense_n

Output :  max_sense ( dot(emb_context,emb_sense_1), ..., dot(emb_context,emb_sense_n))

```

Main takeways: some idea about the word Disambiguation task, and the fact that providing definitions and not just the context also helps. For training efficiency encoding them separately is a good idea ( for transformers ) and this approach helps them perform better on zero shot learning tasks, few shot learning tasks and low frequency words. Just curious, that the long tail comparisons haven't been done with glossBERT, when it was the closest to in performance for the normal study...

I'm not sure if this'll work for non common  technical words - can definitely go look up the dataset to see how it actually works

#### Paper 2: Theoretical Limitations of Self-Attention in Neural Sequence Models

I liked this paper mostly because I have like a very very brief idea about formal languages, and handling of no limit recursion and negation // regular languages is something that my manager has spoken about to us at multiple occasions. This paper is interesting, as from a theoretical standpoint it explains how transformers are unable to model "[regular languages](https://stackoverflow.com/questions/6718202/what-is-a-regular-language)."

* this paper looks at whether transformers can look at unbounded hierarchical structures.
* Any type of RNN can model this behavior - parity and closed brackets, this is checked by looking at tasks Dyck2, and parity ..
* The authors describe some theoretical ways in which they prove that transformers with either soft or hard attention cannot model parity or dyck2 - concluding that they cannot model stacks or hierarchies or finite state automata.
* Natural languages can be approximated well with a model which is weak for formal languages.
* While transformers can model language well, they don't encode generalizations, and does low perplexity even mean understanding?

Some question and answers that were interesting:
Q: Do you think the inability to model formal languages that you investigated has any bearing on the standard tasks in NLP used to evaluate transformers in any way? As in, do you feel there might be some improvements that could be made with model evaluation?
A: in naturalistic data, depth of recursion is never really very high, and thus evaluation based on average performance might miss these limitations at modeling recursion, because, in any naturalistic task, very few inputs would be challenging enough to bring out those limitations. Adversarial evaluation methods (such as ANLI),inguistic challenge sets (like Linzen et al 2015) and contrast sets (Kaushik et al 2020, Gardner et al 2020) might be able to probe such limitations more directly, because they put more weight on "getting everything right", not just getting most things right "on average".

related paper: https://virtual.acl2020.org/paper_main.543.html

#### Paper 3 : Knowledge Graph Embedding Compression

* Representations of KGs to improve generalization and robustness in downstream tasks.
* embeddings in continuous vector spaces, entities - learning continuous vectors, and each relation is an operation in the same space. Then you correctly score the triple (ei, r, ej), with a contrastive loss.
* major issue: number of embedding parameters grows linearly with the # of entities. Problem with large knowledge graphs, sparse entities and relations, lots of redundancies as well since many entities are similar to each other
* Learn discrete KG representations, entity will be a sequence of "d" codes where each code can take values in 1-k. Can capture semantics in that case.. coding scheme is compact as compared to the continuous representations.
* Autoencoder approach
```
input : continuous approach
There are "k" key vectors
argmin_k dist(continuous_rep, all_keys)
output : d dimensional discrete representation
```

* discretization process: the above is done via two approachs: vector quantization, tempering softmax
* reverse process: either via a lookup table or via a non linear LSTM based approach

### Paper 4: Pretrained Transformers Improve Out-of-Distribution Robustness
* very simple but insightful paper as to evaluate how well transformer based methods deal with OOD datapoints.
* In the paper, we see that pretrained models improve OOD generalization, and OOD detection. ( it was TIL to see the difference between generalization and detection .. ).
* For generalization used : STS
* just because a model is larger, doesn't mean that it'll perform well on OOD detection. But they can train on more data which seems to improve robustness.

---

### 5th July 2020

#### Mostly ACL notes

#### Tutorial notes:
* interesting research question: interpretability analysis of multimodal documents. How do representations learn from different modalities?
* Only speech analysis: some things are directly applicable from images, because they are also continuous signals -  saliency maps, spectrograms etc.
* work on interpretability on formal languages. trade off between the interpretability of the model and it's quality, you can constraint the model and gain interpretability but might lose the learning power..
* What we want, ultimately is for interpretability to improve the model, and not constrain it..
* In some situations interaction between the system and the user is important, whereas in other cases it might not be desirable or possible.. in a radiology report you'd want it to be a interactive, but for an auto-driving car, you'd want the model to be very sure and maximize performance instead of having "doubts"
* In NLP the issue of learning can be multi-step: model, data, the human language is not supposed to learn it? from the scientific point as well as from a linguistic perspective..
* how infinitely recursive should language be??
* Question:which probing classifier to use? is it fair to compare two classifiers with different sizes?
** Idea: of using the minimum description length tells you not only the performance of the probe, but also how difficult it is to solve the task, a measure on how challenging it is to do?
* human error patterns vs model error patterns : any work? how do you differentiate - and how do you use them to make your model more faithful? before you fix the mistakes, you need to see how they arise. * look at generation track papers at ACL
* Brilliant BERT visualization demo [here](https://exbert.net/) [IMPORTANT]
* Another point for probing classifiers was to learn the "latent variable" that causes the model to perform well i.e. more in terms of the control experiments.
* what about carefully controlled test sets as used in psycho-linguistics human studies? can you transfer these evaluations to models?  
** idea is to eventually use these small sets. Look up paper on : posing fair generalization tasks, emnlp 2019. With small corpus, you get 1 or 0 answers as to whether the model does good on it or not, whereas with numbers on bigger corpus it's very difficult to gauge which quality of the model is being assessed. But again the trick is to ensure the very small targeted corpus is created properly?
* Q: What does it even mean to say that "a pretrained LM knows a task at 80%"? to me it sounds ;like the model DOES NOT know the phenomena :
** We want to go from not knowing it, to knowing it a little, saturating on the information. Similar to how it's in humans, kids might be not knowing a phenomena, and then they go on to knowing it properly. In models, the all or nothing doesn't happen, so how do you say is "80%" good enough? On going from 80 - 81% you dont know what the model learned in that 1%.... VERY VAGUE HERE.
* thoughts on probing for factual knowledge? decouple what the model can be trained on vs what the model can learn .. decouple memorization from reasoning .. memorization vs inference. The model needs to be more than a look up table, and more than just a replacement for a KB.
* What is your thought about configuring probes for fairness or privacy-preserving purposes, e.g. to show a model lacks interpretability wrt to gender or other protected attributes when it's trained to some reasonable degree for some designated prediction task?
** Very important, but how do you define biases? some exist in society and it is very difficult to get rid of them. It's definitely a good use case (look up references). Connects probing with the behavior side of inference.

#### Session 2 :
* disagreements in human judgements, how to handle these? inferences about meaning when there is no clear cut answer .. hard to say what behavior our model is supposed to have given these uncertainties.. challenge on how to design these sets
* do we always want to mimic human behavior? depends on the goal of the user creating the system .. eg: dialogue systems are difficult to evaluate, but for something like question answering you can still be more objective and try and achieve "super human" performance
* Q: Interpretability & subwords: how (if) do you think the fact that SOTA contextualization models use byte pairs affects their interpretability when compared to "standard" LSTMs + word-tokens
** we as humans don't use subwords and these don't necessarily adhere to the morphological information, and that makes it a little bit harder to interpret ( from a visualization perspective ). For behaviourial analysis : ends up being a part of the black box, the advantage of subwords is in the multi-lingual languages ( moving away from white space tokenization ). Investigation of how these different subwords change how the models performs ( subpiece vs word piece etc ).
* Q: Do any of you have thoughts like these? Are you tempted by models which can be better "guided"?  Any insights?
** with models now you try and build tasks into a model, which is mostly human guided < look up more about it later READ >
* Q : Which type of interpretability method (like post-hoc or model based) should be better for educational NLP areas like essay scoring, etc.
** Is the audience the engineers or the students? if it's the first, everything still remains the same, other than the document based models. If the audience is the students - still nervous using the tools - so mostly visualizations. The structural and behavior based methods are more for the machine learning engineers. Check for systematic biases while scoring students. What type of explanations to show to the students? ( from a user perspective )
* Q: Have you seen any work performing probing studies on document representation (say from BERT)?
** Not really. looked at discord representations, generation settings to say what are they sensitive to .. what sort of coherence chains to they have or not have .. You need adaptation to the model though.
* Q: How does one analyze a model architecture which transitions in terms of "what" it does [in terms of various linguistic properties] with increasing or decreasing training/finetuning sizes? [e.g it encodes property z only after a certain threshold of training examples N_min, or perhaps worse still, the behaviour is non-monotonic w.r.t encoding z]
** maybe by looking at different checkpoints, but at different points in training, diff datasets etc. Very complex to look at - can't just look at it post-hoc as an exploratory analysis.. probably better driven as hypothesis testing ( eg: Certain linguistic understanding gets better with training time etc )
* What's your thought on recent debiasing methods that work by downweighting potentially biased examples in the training data? They work well in improving the performance on some challenge datasets (e.g., HANS). But since new types of bias are still being discovered and these methods usually target a specific 'known' bias -- do you think these methods are the way to go for a better and more robust model of language?
** Similar to data augmentation, but skeptical of these kinds of methods - the concern is that these are too tight to the probing set - we need a much fuller picture of what did the downweighting change, but we need to look at a more holistic picture, what other heuristics it might be adapting.
* Q:Applying current probing methods to long documents?
* Analysis methods are limited by common tasks in NLP, and there aren't many common tasks looking at full documents. Generally stay in sentence level, or paragraph levels. The only popular tracks are summarization.. It is intrinsically much harder to do this cause you don't know how to fully interpret a model.

#### Second tutorial :
[Common Sense Tutorial](https://homes.cs.washington.edu/~msap/acl2020-commonsense/)

##### Common sense knowledge in pre-trained models: ( generic common sense )
* Q: Do pretrained models already capture common sense knowledge? ( only during pretraining ) : there is some evidence from KB completion tasks // BERT performs better on one-to-one answers as compared to many-to-many, and doesn't perform as well as supervised methods ( expected ).
* Weir et al, 2020 - properties of concepts: can the LMs distinguish "concepts"? Models perform better on encyclopedic / functional knowledge as compared to visual or perceptual knowledge. Difficult to learn these alone from text. Positive evidence that there is knowledge in the LMs.
* Can we trust these models? Not always maybe ..
* Zero shot setup for LM common sense training vs knowledge informed model
* knowlede informed model: with each question, enrich it with more information from conceptnet, ngrams, come. Improves the model but clarifications are not useful as per humans
* good performance is due to knowledge in LM or training the large model with more data?
* Do NLM representations learn physical commonsense?
* mitigate reporting bias? look at surprising outcomes etc/ [read van durme's paper]

#### Common sense resources / existing efforts to distill knowledge in the current models
* "mental models" [graesser 1994] + personal experiences + world knowledge and common sense
* common sense: bank of knowledge that the models can use
* existing resouces : cyc ( lenat et al, 1984 ) - human like common sense reasoning and develop new rules etc . Developed their own ontology and language with new concepts and a reasoning engine
* conceptnet? : [ question : using concept net for entity linking or hierarchical entity linking ] has general common sense knowledge, multi-lingual. Question is concept net multi-modal? this is more semantic knowledge
* atomic: causes and effects of action triples, more inferential knowledge
* extracting information:read and parse, create rules, and filter them..
* framenet?

#### NN models for common sense reasoning :
* KB + text -> model -> output ( informed models )
* incorporating external knowledge into Neural Models
* task : what task are you solving and if you need external knowledge? ( story telling, machine comprehension etc)
* KB : where are you getting your knowledge source ( conceptnet, atomic, wordnet, sentiwordnet, cyc, mining, handcrafted rules etc )
* neural component
* combine information sources? ( scoring function, convert symbolic to vector representation (+ attention ), multi task learning )

Q Does fine tuning make the LM unlearn the common sense it learnt during pretraining? "catastrophic forgetting"

#### Neuro symboli representations of commonsense knowledge

* Limitations of knowledge graph:
* very rarely do you find the same query as is, so you end up return knowledge that's incorrect or noisy
* problem with common sense knowledge is that it's often implicit and not really written down. So you started using big graphs like conceptnet
* learning structure of knowledge: is to < head, relation, target> make the language model ( now known as "knowledge model" ) to generate the target, after seeing head and relation // fine tune a pretrained model to the above training style
* common sense transformers OR "COMET for short
* Comet model : atomic knowledge graph // transfer learning from language


---
### 4th July 2020

#### Mostly ACL notes
continuing on my TILs from "Interpretability and Analysis in Neural NLP" hosted by Yonatan Belinkov, Sebastian Gehrmann and Ellie Pavlick...


#### behavior analysis
the second way of understanding interpretability of these models is to a behavior analysis. In this method, you tend to look at  data points that are statistically less probable to be seen my the model during training, and then you try and see how your model generalizes to these points. This approach is very different from the probing or structural analysis approach.
* Some advantages: it's algorithm agnostic, practical, has an objective criteria for evaluating the representations.
* disadvantage: 1. if the model doesn't perform well on the curated dataset - do you debug the model or the data it's seen? 2. what level of generalization is considered to be fair? 3. you technically don't get much insights about WHY the model failed.

#### visualizations :
The third method for interpretability is via interfaces and visualizations. You try and understand the larger patterns by filtering noise.. this can help understand concepts in higher dimensions. It can be used for reducing exploration space ( eg; in model selection ), understand data, features, as well as important aspects of the modeling concepts ( eg: attention layer )

Can look at it in three perspectives:
* task : model selection , model decisions
* user : architect ( who sees just the model ), trained ( model + data ), end-user ( only sees the application )
* model involvement : interactive visualizations( with has the ability to change model parameters, links via interface ) , passive involvement ( eg: in tensorboard you only see the statistics, graphs etc but have no control over the actual underlying model  )

* difficulties: takes a lot of time and effort to build these interfaces especially to scale over multiple usecases
*  open research questions :
* how do these visualizations improve downstream trust?
* how do these insights improvement the model ? ( eg: visualizing the outliers )
* how to help understand causality?
* how to develop model interactive generic interfaces?

---

### 3rd July 2020
#### Mostly ACL notes
So I haven't posted in a while, because I was attending two conferences along with office work. I'll try and be more diligent this week, and write about my TILs regularly.  

In the coming week I'm attending ACL 2020! Seems very overwhelming given the number of tutorials, workshops and the sheer number of papers that have been accepted!  A lot of time this week has gone into trying to figure out what sessions I want to attend. Since the tutorials are recorded, I went through the tutorial on "Interpretability and Analysis in Neural NLP" hosted by Yonatan Belinkov, Sebastian Gehrmann and Ellie Pavlick.
Main takeaways:
* This tutorial focused on three ways for Interpretability : structural analysis,
* The first part of the talk focused on structural analysis via "probing classifier".

#### probing classifier

TLDR for a probing classifier:
```
Input: x, linguistic_ann(x)
F: f(x) -> rep(x)
G: g(rep(x)) -> g_output
evaluation g_output with linguistic_ann(x)
```
* In this model, you basically evaluate F, and the representations generated by model F on a dataset with has some linguistic annotations. Another model "g" is then trained using these representations to see how it performs on the same dataset. Model g is trained in a way to maximize the mutual information between the representations and the output annotations
* on looking at the performance of "f" i.e. representations from intermediate layers of "f" , we can conclude a hierarchy of tasks that each level would represent better.
* i.e. the lower layers of any NN model learn  more simple features like morphology, POS where as the upper layers can capture more complex concepts like dependencies, syntax and semantics etc. ( similar to as seen in CNNs for vision for example)
* shortcomings of this approach:
** even though we are comparing rep(x), but the performance is also based on how to choose "g", so the more complex "g" you choose, the better you are able to use the representation for the linguistic task. how does that play into comparing different "f"?
**  you need to have some standard models of "g" for comparison to say how the representations stand relative to a baseline
** the model "g" is being trained on a linguistic task which may be different from the actual task on which "f" is trained so even though "f" is performing well on the probing task, it might not perform well on the actual downstream task, how to resolve this?
** difference between "accuracy" and "selectivity" <--- (read about selectivity)  

continued ..
