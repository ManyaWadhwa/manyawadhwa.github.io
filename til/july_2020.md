---
layout: default
title: TIL/July 2020
---

[Go to the main TIL page](https://manyawadhwa.github.io/til/)

### 10th July 2020
Workshop day 2 at ACL! Still compiling my notes...

### 9th July 2020
Workshop day 1 at ACL! Still compiling my notes...

### 8th July 2020
Third day of ACL! Still compiling my notes...

### 7th July 2020

Reading List for this day is mentioned [here](july_2020_acl_papers_list.md)

#### Paper 1 : [BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance](https://www.aclweb.org/anthology/2020.acl-main.368.pdf)
-- initially I didn't know that this concept by the same authors has already been submitted to AAAI'2020. Interesting. But nice! Maybe they did some more work on top of their simple attentive mimicking algorithm
*  BERTRAM : BERT for attentive mimicking - understand rare words better
* Dataset rarification : transform datasets such that rare words are important for solving the task.
* the main idea of the paper is that because of the way BERT tokenizes the words there is a loss of information, which makes it difficult for it to perform on rare words.. eg: unicycle will be broken into : un, #ic, ##y, ##cle so the authors argue that there is a loss of information in representing the word in a distributed manner, and instead should be represented initially in a non contextualized way!
* One of the ways these can be used as input ( non contexualized embedding ) eg: "the child is riding a un #ic ##y ##cle / unicycle"
* So instead of passing just the tokenized word or full word - you can "/" it and send.
* QUESTION:how do you identify a word to be "rare" from an inference perspective? Answer from the chat: any word that occurs < 100 times in the BERTs pretraining corpus can be considered as rare..
* Model: to get the representation of the rare word, you [mask] it in the input sentence, send the sentence through BERT to predict the word, and pass it through a linear model to get an embedding. In the input you also send "ngrams"  of the word to have "form" information.

```
For one word in one context:
Form Model : form([word_nrgams]) -> v_form
BERT : bert([v_form, w_1, w_2...[mask],..w_n]) -> v_word_in_context

For one word in multiple contexts:
Attentive Mimicking: attention([v_w_in_c1,v_w_in_c2 ... v_w_in_cn]) -> v_final
```

* Attentive Mimicking : is basically a linear combination of all the context embeddings, where the weights are determined using self-attention.
* How to train? Use words for which we know BERT performs well, and pass it through the BERTRAM model - where the loss function is the difference in the embeddings produced by the model and the gold standard embedding.
```
loss_function = BERTRAM_embedding - gold_standard_embedding
```

* they say the model performs well as long as the context is not more than 128 words - why this limit?
* relevant papers from the chat - https://arxiv.org/abs/2005.04611

#### Paper 2:[SenseBERT: Driving Some Sense into BERT](https://www.aclweb.org/anthology/2020.acl-main.423.pdf)
* Driving some sense into BERT
* Infusing more semantics in NN
* BERT is pretrained model - MLM
* But even though BERT is contextualized it is not able to capture super granular sense - the same word can have multiple meanings i.e. "senses"
* trained a masked sense prediction task along with masked word prediction* use wordnet to induce weak supervision - along with noun - what sort of a noun is this? noun - person, food etc LEXICAL CATEGORIZATION TASK
* wordnet gives a "word to sense" mapping
* "output input weight tying trick" : [paper](https://arxiv.org/pdf/1611.01462v3.pdf)
* does the model just learn these senses? the masking and training is done in a way that the model's understanding on the sense is re-inforced by showing the same word in multiple senses and by showing the same sense for multiple words/context


#### Paper 3: [SpanBERT: Improving Pre-training by Representing and Predicting Spans](https://virtual.acl2020.org/paper_tacl.1853.html)

I think I attended Danqi's talk at AAAI at a NLP for Question Answering workshop where she spoke about this work!
* Designing more efficient pretraining task
* Why do we need spans? - global named entities / local or domain specific meaning.
* Span representations are also ubiquitous in NLP - QA, relation extraction, coreference resolution etc
* Pretraining with Span representations : contiguous masking spans,SBO - span boundary objective , single sequence document level input pipeline
* the objective function now looks like the following :
```
loss(word) = loss_mlm(word) + loss_sbo(word)
loss_sbo = -log p(word/span_start, span_end, relative_pos_of_word)
```
* do the above for each span word
* trying to encode the span into it's end points
* single sequence inputs compared to bi sequence inputs in BERT
```
single sequence input : [cls] <input_sequence> [sep]
bi sequence pipeline : [cls] <sent> [sep] <sent> [sep]
```
* the second one might lead to conditioning on noise ( why and how )- because it is randomly sampling from the document ..
* document level sequence input provides more context for longer spans

---

### 6th July 2020

Reading List for this day is mentioned [here](july_2020_acl_papers_list.md)

The main conference ACL started on the 6th of July! Very overwhelming, but in a way nice cause it gave me access to A LOT  of resources and mentoring sessions.

Some highlights:

#### keynote session by Kathleen McKeown
* She spoke about language generation - past present and future.
* Aspects of past relevant in the deep learning age?
* language generation can be : data, summarization, dialog, and machine translation ( interesting and true! )
* Neural Nets are the only way in which we know how to approximate a non linear function.
* Pre deep learning approaches: in the 1990s papers in generation focused on learning like the use one word - which is very interesting ( eg: use of "now" in the temporal sense vs the use of "now" in the discourse sense ), the evaluation wasn't as intense as it is now these days, it was done by looking and manually looking at the output.. then evaluation moved to developing annotations and corporas.
* Data - all the variation is in the long tail distribution, but mostly everyone ends up evaluating on the head , Original vision on frame semantics are not just semantic role labeling but included a deeper pragmatic understanding  
* Applications for which deep learning is not suited: < personaly for me these were the most interesting highlights ) - constraints on choice in language generation, looking at data, learning from other disciplines, and SOLVING PROBLEMS THAT MATTER AND NOT JUST PROBLEMS FOR WHICH DATA IS AVAILABLE.
* interesting ideas: controlled generation, include constraints while generating language, generating the "plan" first and then generating the language from it - so more like a prototype and then you actually take the path, old-fashioned AI dialogue planning - in this context you can produce x dialogues, how to you deal with spurious correlations, using "symbolic" or "semantic" logic, understanding causality is very easy to understand for humans, events in time WITHOUT having to update the models with weeks of training - easy update to the knowledge, solving a particular task and not just a dataset seems to be a huge challenge still..

#### Papers

SO the first day of ACL has many many many papers, I tried to go through some of these so these are the initial set of notes, will go through the others properly in some time:

#### Paper 1: [Moving Down the Long Tail of Word Sense Disambiguation with Gloss-Informed Biencoders](https://arxiv.org/abs/2005.02590)

One of the reasons this paper seemed interesting was because in the abstract it stated the following "word senses are not uniformly distributed, causing existing models to generally perform poorly on senses that are either rare or unseen during training."

Which even though we moved from word2vec to more contextual embeddings, the above is a problem that that even contextual models face..

* Word Sense Disambiguation task : Given a word and the context, choose the appropriate definition.
* word senses have a zipfian distribution ( read )
* data imbalance because of uncommon words..
* capture rare sense with pretrained models and "glosses"
* So there has already been work done with glossBERT, or having a neural encoder for the gloss BUT shortcomings
* Gloss informed Bi-encoder : encode the context and gloss independently and align the target word embedding to the correct sense embedding - biencoder takes two independent BERTs and encodes the context as well as the gloss independently. More efficient than 'cross encoding' since it the transformer performance is quadratic w.r.t the length of the input..

```
Context Encoder : f(input: context + word)-> emb_context
Gloss Encoder : f(definitions) -> emb_sense_1, emb_sense_2 ... emb_sense_n

Output :  max_sense ( dot(emb_context,emb_sense_1), ..., dot(emb_context,emb_sense_n))

```

Main takeways: some idea about the word Disambiguation task, and the fact that providing definitions and not just the context also helps. For training efficiency encoding them separately is a good idea ( for transformers ) and this approach helps them perform better on zero shot learning tasks, few shot learning tasks and low frequency words. Just curious, that the long tail comparisons haven't been done with glossBERT, when it was the closest to in performance for the normal study...

I'm not sure if this'll work for non common  technical words - can definitely go look up the dataset to see how it actually works

#### Paper 2: Theoretical Limitations of Self-Attention in Neural Sequence Models

I liked this paper mostly because I have like a very very brief idea about formal languages, and handling of no limit recursion and negation // regular languages is something that my manager has spoken about to us at multiple occasions. This paper is interesting, as from a theoretical standpoint it explains how transformers are unable to model "[regular languages](https://stackoverflow.com/questions/6718202/what-is-a-regular-language)."

* this paper looks at whether transformers can look at unbounded hierarchical structures.
* Any type of RNN can model this behavior - parity and closed brackets, this is checked by looking at tasks Dyck2, and parity ..
* The authors describe some theoretical ways in which they prove that transformers with either soft or hard attention cannot model parity or dyck2 - concluding that they cannot model stacks or hierarchies or finite state automata.
* Natural languages can be approximated well with a model which is weak for formal languages.
* While transformers can model language well, they don't encode generalizations, and does low perplexity even mean understanding?

Some question and answers that were interesting:
Q: Do you think the inability to model formal languages that you investigated has any bearing on the standard tasks in NLP used to evaluate transformers in any way? As in, do you feel there might be some improvements that could be made with model evaluation?
A: in naturalistic data, depth of recursion is never really very high, and thus evaluation based on average performance might miss these limitations at modeling recursion, because, in any naturalistic task, very few inputs would be challenging enough to bring out those limitations. Adversarial evaluation methods (such as ANLI),inguistic challenge sets (like Linzen et al 2015) and contrast sets (Kaushik et al 2020, Gardner et al 2020) might be able to probe such limitations more directly, because they put more weight on "getting everything right", not just getting most things right "on average".

related paper: https://virtual.acl2020.org/paper_main.543.html

#### Paper 3 : Knowledge Graph Embedding Compression

* Representations of KGs to improve generalization and robustness in downstream tasks.
* embeddings in continuous vector spaces, entities - learning continuous vectors, and each relation is an operation in the same space. Then you correctly score the triple (ei, r, ej), with a contrastive loss.
* major issue: number of embedding parameters grows linearly with the # of entities. Problem with large knowledge graphs, sparse entities and relations, lots of redundancies as well since many entities are similar to each other
* Learn discrete KG representations, entity will be a sequence of "d" codes where each code can take values in 1-k. Can capture semantics in that case.. coding scheme is compact as compared to the continuous representations.
* Autoencoder approach
```
input : continuous approach
There are "k" key vectors
argmin_k dist(continuous_rep, all_keys)
output : d dimensional discrete representation
```

* discretization process: the above is done via two approachs: vector quantization, tempering softmax
* reverse process: either via a lookup table or via a non linear LSTM based approach

### Paper 4: Pretrained Transformers Improve Out-of-Distribution Robustness
* very simple but insightful paper as to evaluate how well transformer based methods deal with OOD datapoints.
* In the paper, we see that pretrained models improve OOD generalization, and OOD detection. ( it was TIL to see the difference between generalization and detection .. ).
* For generalization used : STS
* just because a model is larger, doesn't mean that it'll perform well on OOD detection. But they can train on more data which seems to improve robustness.

---

### 5th July 2020

#### Mostly ACL notes

#### Tutorial notes:
* interesting research question: interpretability analysis of multimodal documents. How do representations learn from different modalities?
* Only speech analysis: some things are directly applicable from images, because they are also continuous signals -  saliency maps, spectrograms etc.
* work on interpretability on formal languages. trade off between the interpretability of the model and it's quality, you can constraint the model and gain interpretability but might lose the learning power..
* What we want, ultimately is for interpretability to improve the model, and not constrain it..
* In some situations interaction between the system and the user is important, whereas in other cases it might not be desirable or possible.. in a radiology report you'd want it to be a interactive, but for an auto-driving car, you'd want the model to be very sure and maximize performance instead of having "doubts"
* In NLP the issue of learning can be multi-step: model, data, the human language is not supposed to learn it? from the scientific point as well as from a linguistic perspective..
* how infinitely recursive should language be??
* Question:which probing classifier to use? is it fair to compare two classifiers with different sizes?
** Idea: of using the minimum description length tells you not only the performance of the probe, but also how difficult it is to solve the task, a measure on how challenging it is to do?
* human error patterns vs model error patterns : any work? how do you differentiate - and how do you use them to make your model more faithful? before you fix the mistakes, you need to see how they arise. * look at generation track papers at ACL
* Brilliant BERT visualization demo [here](https://exbert.net/) [IMPORTANT]
* Another point for probing classifiers was to learn the "latent variable" that causes the model to perform well i.e. more in terms of the control experiments.
* what about carefully controlled test sets as used in psycho-linguistics human studies? can you transfer these evaluations to models?  
** idea is to eventually use these small sets. Look up paper on : posing fair generalization tasks, emnlp 2019. With small corpus, you get 1 or 0 answers as to whether the model does good on it or not, whereas with numbers on bigger corpus it's very difficult to gauge which quality of the model is being assessed. But again the trick is to ensure the very small targeted corpus is created properly?
* Q: What does it even mean to say that "a pretrained LM knows a task at 80%"? to me it sounds ;like the model DOES NOT know the phenomena :
** We want to go from not knowing it, to knowing it a little, saturating on the information. Similar to how it's in humans, kids might be not knowing a phenomena, and then they go on to knowing it properly. In models, the all or nothing doesn't happen, so how do you say is "80%" good enough? On going from 80 - 81% you dont know what the model learned in that 1%.... VERY VAGUE HERE.
* thoughts on probing for factual knowledge? decouple what the model can be trained on vs what the model can learn .. decouple memorization from reasoning .. memorization vs inference. The model needs to be more than a look up table, and more than just a replacement for a KB.
* What is your thought about configuring probes for fairness or privacy-preserving purposes, e.g. to show a model lacks interpretability wrt to gender or other protected attributes when it's trained to some reasonable degree for some designated prediction task?
** Very important, but how do you define biases? some exist in society and it is very difficult to get rid of them. It's definitely a good use case (look up references). Connects probing with the behavior side of inference.

#### Session 2 :
* disagreements in human judgements, how to handle these? inferences about meaning when there is no clear cut answer .. hard to say what behavior our model is supposed to have given these uncertainties.. challenge on how to design these sets
* do we always want to mimic human behavior? depends on the goal of the user creating the system .. eg: dialogue systems are difficult to evaluate, but for something like question answering you can still be more objective and try and achieve "super human" performance
* Q: Interpretability & subwords: how (if) do you think the fact that SOTA contextualization models use byte pairs affects their interpretability when compared to "standard" LSTMs + word-tokens
** we as humans don't use subwords and these don't necessarily adhere to the morphological information, and that makes it a little bit harder to interpret ( from a visualization perspective ). For behaviourial analysis : ends up being a part of the black box, the advantage of subwords is in the multi-lingual languages ( moving away from white space tokenization ). Investigation of how these different subwords change how the models performs ( subpiece vs word piece etc ).
* Q: Do any of you have thoughts like these? Are you tempted by models which can be better "guided"?  Any insights?
** with models now you try and build tasks into a model, which is mostly human guided < look up more about it later READ >
* Q : Which type of interpretability method (like post-hoc or model based) should be better for educational NLP areas like essay scoring, etc.
** Is the audience the engineers or the students? if it's the first, everything still remains the same, other than the document based models. If the audience is the students - still nervous using the tools - so mostly visualizations. The structural and behavior based methods are more for the machine learning engineers. Check for systematic biases while scoring students. What type of explanations to show to the students? ( from a user perspective )
* Q: Have you seen any work performing probing studies on document representation (say from BERT)?
** Not really. looked at discord representations, generation settings to say what are they sensitive to .. what sort of coherence chains to they have or not have .. You need adaptation to the model though.
* Q: How does one analyze a model architecture which transitions in terms of "what" it does [in terms of various linguistic properties] with increasing or decreasing training/finetuning sizes? [e.g it encodes property z only after a certain threshold of training examples N_min, or perhaps worse still, the behaviour is non-monotonic w.r.t encoding z]
** maybe by looking at different checkpoints, but at different points in training, diff datasets etc. Very complex to look at - can't just look at it post-hoc as an exploratory analysis.. probably better driven as hypothesis testing ( eg: Certain linguistic understanding gets better with training time etc )
* What's your thought on recent debiasing methods that work by downweighting potentially biased examples in the training data? They work well in improving the performance on some challenge datasets (e.g., HANS). But since new types of bias are still being discovered and these methods usually target a specific 'known' bias -- do you think these methods are the way to go for a better and more robust model of language?
** Similar to data augmentation, but skeptical of these kinds of methods - the concern is that these are too tight to the probing set - we need a much fuller picture of what did the downweighting change, but we need to look at a more holistic picture, what other heuristics it might be adapting.
* Q:Applying current probing methods to long documents?
* Analysis methods are limited by common tasks in NLP, and there aren't many common tasks looking at full documents. Generally stay in sentence level, or paragraph levels. The only popular tracks are summarization.. It is intrinsically much harder to do this cause you don't know how to fully interpret a model.

#### Second tutorial :
[Common Sense Tutorial](https://homes.cs.washington.edu/~msap/acl2020-commonsense/)

##### Common sense knowledge in pre-trained models: ( generic common sense )
* Q: Do pretrained models already capture common sense knowledge? ( only during pretraining ) : there is some evidence from KB completion tasks // BERT performs better on one-to-one answers as compared to many-to-many, and doesn't perform as well as supervised methods ( expected ).
* Weir et al, 2020 - properties of concepts: can the LMs distinguish "concepts"? Models perform better on encyclopedic / functional knowledge as compared to visual or perceptual knowledge. Difficult to learn these alone from text. Positive evidence that there is knowledge in the LMs.
* Can we trust these models? Not always maybe ..
* Zero shot setup for LM common sense training vs knowledge informed model
* knowlede informed model: with each question, enrich it with more information from conceptnet, ngrams, come. Improves the model but clarifications are not useful as per humans
* good performance is due to knowledge in LM or training the large model with more data?
* Do NLM representations learn physical commonsense?
* mitigate reporting bias? look at surprising outcomes etc/ [read van durme's paper]

#### Common sense resources / existing efforts to distill knowledge in the current models
* "mental models" [graesser 1994] + personal experiences + world knowledge and common sense
* common sense: bank of knowledge that the models can use
* existing resouces : cyc ( lenat et al, 1984 ) - human like common sense reasoning and develop new rules etc . Developed their own ontology and language with new concepts and a reasoning engine
* conceptnet? : [ question : using concept net for entity linking or hierarchical entity linking ] has general common sense knowledge, multi-lingual. Question is concept net multi-modal? this is more semantic knowledge
* atomic: causes and effects of action triples, more inferential knowledge
* extracting information:read and parse, create rules, and filter them..
* framenet?

#### NN models for common sense reasoning :
* KB + text -> model -> output ( informed models )
* incorporating external knowledge into Neural Models
* task : what task are you solving and if you need external knowledge? ( story telling, machine comprehension etc)
* KB : where are you getting your knowledge source ( conceptnet, atomic, wordnet, sentiwordnet, cyc, mining, handcrafted rules etc )
* neural component
* combine information sources? ( scoring function, convert symbolic to vector representation (+ attention ), multi task learning )

Q Does fine tuning make the LM unlearn the common sense it learnt during pretraining? "catastrophic forgetting"

#### Neuro symboli representations of commonsense knowledge

* Limitations of knowledge graph:
* very rarely do you find the same query as is, so you end up return knowledge that's incorrect or noisy
* problem with common sense knowledge is that it's often implicit and not really written down. So you started using big graphs like conceptnet
* learning structure of knowledge: is to < head, relation, target> make the language model ( now known as "knowledge model" ) to generate the target, after seeing head and relation // fine tune a pretrained model to the above training style
* common sense transformers OR "COMET for short
* Comet model : atomic knowledge graph // transfer learning from language


---
### 4th July 2020

#### Mostly ACL notes
continuing on my TILs from "Interpretability and Analysis in Neural NLP" hosted by Yonatan Belinkov, Sebastian Gehrmann and Ellie Pavlick...


#### behavior analysis
the second way of understanding interpretability of these models is to a behavior analysis. In this method, you tend to look at  data points that are statistically less probable to be seen my the model during training, and then you try and see how your model generalizes to these points. This approach is very different from the probing or structural analysis approach.
* Some advantages: it's algorithm agnostic, practical, has an objective criteria for evaluating the representations.
* disadvantage: 1. if the model doesn't perform well on the curated dataset - do you debug the model or the data it's seen? 2. what level of generalization is considered to be fair? 3. you technically don't get much insights about WHY the model failed.

#### visualizations :
The third method for interpretability is via interfaces and visualizations. You try and understand the larger patterns by filtering noise.. this can help understand concepts in higher dimensions. It can be used for reducing exploration space ( eg; in model selection ), understand data, features, as well as important aspects of the modeling concepts ( eg: attention layer )

Can look at it in three perspectives:
* task : model selection , model decisions
* user : architect ( who sees just the model ), trained ( model + data ), end-user ( only sees the application )
* model involvement : interactive visualizations( with has the ability to change model parameters, links via interface ) , passive involvement ( eg: in tensorboard you only see the statistics, graphs etc but have no control over the actual underlying model  )

* difficulties: takes a lot of time and effort to build these interfaces especially to scale over multiple usecases
*  open research questions :
* how do these visualizations improve downstream trust?
* how do these insights improvement the model ? ( eg: visualizing the outliers )
* how to help understand causality?
* how to develop model interactive generic interfaces?

---

### 3rd July 2020
#### Mostly ACL notes
So I haven't posted in a while, because I was attending two conferences along with office work. I'll try and be more diligent this week, and write about my TILs regularly.  

In the coming week I'm attending ACL 2020! Seems very overwhelming given the number of tutorials, workshops and the sheer number of papers that have been accepted!  A lot of time this week has gone into trying to figure out what sessions I want to attend. Since the tutorials are recorded, I went through the tutorial on "Interpretability and Analysis in Neural NLP" hosted by Yonatan Belinkov, Sebastian Gehrmann and Ellie Pavlick.
Main takeaways:
* This tutorial focused on three ways for Interpretability : structural analysis,
* The first part of the talk focused on structural analysis via "probing classifier".

#### probing classifier

TLDR for a probing classifier:
```
Input: x, linguistic_ann(x)
F: f(x) -> rep(x)
G: g(rep(x)) -> g_output
evaluation g_output with linguistic_ann(x)
```
* In this model, you basically evaluate F, and the representations generated by model F on a dataset with has some linguistic annotations. Another model "g" is then trained using these representations to see how it performs on the same dataset. Model g is trained in a way to maximize the mutual information between the representations and the output annotations
* on looking at the performance of "f" i.e. representations from intermediate layers of "f" , we can conclude a hierarchy of tasks that each level would represent better.
* i.e. the lower layers of any NN model learn  more simple features like morphology, POS where as the upper layers can capture more complex concepts like dependencies, syntax and semantics etc. ( similar to as seen in CNNs for vision for example)
* shortcomings of this approach:
** even though we are comparing rep(x), but the performance is also based on how to choose "g", so the more complex "g" you choose, the better you are able to use the representation for the linguistic task. how does that play into comparing different "f"?
**  you need to have some standard models of "g" for comparison to say how the representations stand relative to a baseline
** the model "g" is being trained on a linguistic task which may be different from the actual task on which "f" is trained so even though "f" is performing well on the probing task, it might not perform well on the actual downstream task, how to resolve this?
** difference between "accuracy" and "selectivity" <--- (read about selectivity)  

continued ..
