---
layout: default
title: Reading List
---

* [Attention is not explanation](https://arxiv.org/pdf/1902.10186.pdf) <- you can find an alternative attention mechanism with a different distribution that gives you the same output.
* [Analyzing Multi-Head Self-Attention:
Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned](https://arxiv.org/pdf/1905.09418.pdf)
* [Under the Hood: Using Diagnostic Classifiers to Investigate and Improve
how Language Models Track Agreement Information](https://arxiv.org/pdf/1808.08079.pdf)
* [Linguistic Analysis of Pretrained Sentence Encoders with Acceptability
Judgments](https://arxiv.org/pdf/1901.03438.pdf)
* [ON THE VARIANCE OF THE ADAPTIVE LEARNING
RATE AND BEYOND](https://arxiv.org/pdf/1908.03265.pdf)
