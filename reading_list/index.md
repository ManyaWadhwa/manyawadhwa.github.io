---
layout: default
title: Reading List
---

https://www.youtube.com/watch?v=8FIEZXMUM2I&t=17s

* [On the Measure of Intelligence](https://arxiv.org/abs/1911.01547)
* [Radioactive data: tracing through training](https://arxiv.org/pdf/2002.00937.pdf)
* ["Transformers as Soft reasoners over language"](https://arxiv.org/abs/2002.05867) by Allen AI
* [Universal transformers](https://arxiv.org/abs/1807.03819)
* http://hanj.cs.illinois.edu/pdf/nips19_ymeng.pdf
* [A Simple Method for Commonsense Reasoning](https://arxiv.org/pdf/1806.02847.pdf)
* [Attention is not explanation](https://arxiv.org/pdf/1902.10186.pdf) <- you can find an alternative attention mechanism with a different distribution that gives you the same output.
* [Analyzing Multi-Head Self-Attention:
Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned](https://arxiv.org/pdf/1905.09418.pdf)
* [Under the Hood: Using Diagnostic Classifiers to Investigate and Improve
how Language Models Track Agreement Information](https://arxiv.org/pdf/1808.08079.pdf)
* [Linguistic Analysis of Pretrained Sentence Encoders with Acceptability
Judgments](https://arxiv.org/pdf/1901.03438.pdf)
* [ON THE VARIANCE OF THE ADAPTIVE LEARNING
RATE AND BEYOND](https://arxiv.org/pdf/1908.03265.pdf)
